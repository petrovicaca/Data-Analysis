# -*- coding: utf-8 -*-
"""Vežba_8_Segmentacija.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1o4t7ZEc3fsjb66tVf3JeTx5oPcDXz-Yk

## Segmentacija korisnika kreditnih kartica metodama mašinskog učenja
"""

url="https://raw.githubusercontent.com/hajdeger/AOP_PUB/master/CC%20GENERAL.csv"

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
from sklearn.cluster import KMeans
from sklearn import preprocessing
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns

td = pd.read_csv(url)

td.head

td.shape

print("Data Types:", td.dtypes)

"""Running the following code tells me that only two features have null values ‘CREDIT_LIMIT’ and ‘MINIMUM_PAYMENTS’. Additionally, less than 5% of each column has nulls. This means that we should be ok to fill these with a sensible replacement value and should still be able to use the feature."""

td.apply(lambda x: sum(x.isnull()/len(td)))

"""The following code fills the missing value with the most commonly occurring value in the column. We could equally use mean or median, or indeed another approach but we will start here and iterate on this if needed."""

td_clean = td.apply(lambda x:x.fillna(x.value_counts().index[0]))  # provera da li su sve nulte vrednosti zamenjene

td_clean.apply(lambda x: sum(x.isnull()/len(td_clean)))

"""I am also going to drop the CUST_ID column as we won’t need this for training."""

cols_to_drop = 'CUST_ID'
td_clean = td_clean.drop([cols_to_drop], axis=1)

print("Data Types:", td_clean.dtypes)

"""## Skaliranje obeležja

The following code scales all the features in the data frame. I am using the min_max_scaler for the first iteration. Different techniques for scaling may have different results.
"""

x = td_clean.values
min_max_scaler = preprocessing.MinMaxScaler()
x_scaled = min_max_scaler.fit_transform(x)
td_cs = pd.DataFrame(x_scaled,columns=td_clean.columns)

"""To illustrate how this looks here are"""

print(td_clean["PURCHASES_FREQUENCY"], td_cs["PURCHASES_FREQUENCY"] )

print(td_clean["BALANCE"], td_cs["BALANCE"] )

"""# Odredjivanje broja klastera"""

Sum_of_squared_distances = []
K = range(1,15)
for k in K:
    km = KMeans(n_clusters=k)
    km = km.fit(td_cs)
    Sum_of_squared_distances.append(km.inertia_)

plt.plot(K, Sum_of_squared_distances, 'bx-')
plt.xlabel('k')
plt.ylabel('Sum_of_squared_distances')
plt.title('Elbow Method For Optimal k')
plt.show()

"""You can see that after 8 clusters adding more gives minimal benefit to the model. I am therefore going to use 8 clusters to train my model.

## Trening

Before training, I am going to divide the data set into a train and test set. The following code divides the data reserving 20% for testing.
"""

np.random.seed(0)
msk = np.random.rand(len(td_cs)) < 0.8
train = td_cs[msk]
test = td_cs[~msk]

"""I then convert both the train and test set into numpy arrays."""

X = np.array(train)
X_test = np.array(test)

"""Next, I call the KMeans fit method using 8 clusters"""

kmeans = KMeans(n_clusters=8, random_state=0).fit(X)

"""Using the trained model I will now predict the clusters on the test set."""

y_k = kmeans.predict(X_test)

y_k.size



"""I’ll now assign the prediction as a new column on the original test data frame to analyse the results."""

test["PREDICTED_CLUSTER"] = y_k

test.shape

test.dtypes

"""## Analysing the clusters

I am going to use the pandas groupby function to analyse a selected number of features for the clusters in order to understand if the model has successfully identified unique segments.
"""

train_summary = test.groupby(by='PREDICTED_CLUSTER').mean()

train_summary.shape



train_summary = test.groupby(by='PREDICTED_CLUSTER').mean()
train_summary = train_summary[['BALANCE', 'PURCHASES', 
                               'PURCHASES_FREQUENCY','CREDIT_LIMIT', 
                               'ONEOFF_PURCHASES_FREQUENCY', 
                              'MINIMUM_PAYMENTS','PRC_FULL_PAYMENT', 
                               'PAYMENTS']]
train_summary

"""Just looking at ‘PURCHASES_FREQUENCY’ we can see that the model has identified some high-frequency purchase segments, clusters 2 and 3. Let’s understand the differences between these two segments to further determine why they are in separate clusters. We can see that cluster 3 has a higher number of total purchases, a higher credit limit, they make frequent one-off purchases and are more likely to pay in full. We can draw the conclusion that these are high-value customers and therefore there will almost certainly be a difference between how you may market to these customers."""

from sklearn.metrics import silhouette_samples, silhouette_score

range_n_clusters = [2, 3, 4, 5, 6]

ss=[]
for k in range_n_clusters:
   km = KMeans(n_clusters=k)
   km = km.fit(X)
   cluster_labels = km.fit_predict(X)
   silhouette_avg = silhouette_score(X, cluster_labels)
   ss.append(silhouette_avg)
   print("For n_clusters =", k,
          "The average silhouette_score is :", silhouette_avg)

plt.plot(range_n_clusters, ss, 'bx-')
plt.xlabel('k')
plt.ylabel('The average silhouette_score')
plt.title('Silhouette Method For Optimal k')
plt.show()

